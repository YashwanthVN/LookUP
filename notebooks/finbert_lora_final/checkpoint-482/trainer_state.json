{
  "best_global_step": 482,
  "best_metric": 0.32365676760673523,
  "best_model_checkpoint": "./finbert_lora_final\\checkpoint-482",
  "epoch": 2.0,
  "eval_steps": 500,
  "global_step": 482,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.04149377593360996,
      "grad_norm": 0.07839247584342957,
      "learning_rate": 2.9626556016597514e-05,
      "loss": 0.5458790302276612,
      "step": 10
    },
    {
      "epoch": 0.08298755186721991,
      "grad_norm": 0.005237376783043146,
      "learning_rate": 2.9211618257261412e-05,
      "loss": 0.48991737365722654,
      "step": 20
    },
    {
      "epoch": 0.12448132780082988,
      "grad_norm": 1.6888333559036255,
      "learning_rate": 2.8796680497925313e-05,
      "loss": 0.35935797691345217,
      "step": 30
    },
    {
      "epoch": 0.16597510373443983,
      "grad_norm": 4.850772380828857,
      "learning_rate": 2.838174273858921e-05,
      "loss": 0.640938663482666,
      "step": 40
    },
    {
      "epoch": 0.2074688796680498,
      "grad_norm": 0.09707718342542648,
      "learning_rate": 2.7966804979253112e-05,
      "loss": 0.3962913274765015,
      "step": 50
    },
    {
      "epoch": 0.24896265560165975,
      "grad_norm": 0.0555955246090889,
      "learning_rate": 2.7551867219917013e-05,
      "loss": 0.08887227773666381,
      "step": 60
    },
    {
      "epoch": 0.29045643153526973,
      "grad_norm": 8.862092018127441,
      "learning_rate": 2.7136929460580915e-05,
      "loss": 0.2729800701141357,
      "step": 70
    },
    {
      "epoch": 0.33195020746887965,
      "grad_norm": 11.323979377746582,
      "learning_rate": 2.6721991701244816e-05,
      "loss": 0.5787821769714355,
      "step": 80
    },
    {
      "epoch": 0.37344398340248963,
      "grad_norm": 5.134685039520264,
      "learning_rate": 2.6307053941908714e-05,
      "loss": 0.46543326377868655,
      "step": 90
    },
    {
      "epoch": 0.4149377593360996,
      "grad_norm": 0.0032711250241845846,
      "learning_rate": 2.5892116182572615e-05,
      "loss": 0.1333439588546753,
      "step": 100
    },
    {
      "epoch": 0.45643153526970953,
      "grad_norm": 0.23824091255664825,
      "learning_rate": 2.5477178423236516e-05,
      "loss": 0.2031480073928833,
      "step": 110
    },
    {
      "epoch": 0.4979253112033195,
      "grad_norm": 0.04118388518691063,
      "learning_rate": 2.5062240663900417e-05,
      "loss": 0.6033592224121094,
      "step": 120
    },
    {
      "epoch": 0.5394190871369294,
      "grad_norm": 7.07978630065918,
      "learning_rate": 2.4647302904564315e-05,
      "loss": 0.5177162647247314,
      "step": 130
    },
    {
      "epoch": 0.5809128630705395,
      "grad_norm": 6.6420979499816895,
      "learning_rate": 2.4232365145228216e-05,
      "loss": 0.2990450382232666,
      "step": 140
    },
    {
      "epoch": 0.6224066390041494,
      "grad_norm": 5.971555233001709,
      "learning_rate": 2.3817427385892114e-05,
      "loss": 0.24831981658935548,
      "step": 150
    },
    {
      "epoch": 0.6639004149377593,
      "grad_norm": 0.0477827787399292,
      "learning_rate": 2.340248962655602e-05,
      "loss": 0.1218605637550354,
      "step": 160
    },
    {
      "epoch": 0.7053941908713693,
      "grad_norm": 4.849154472351074,
      "learning_rate": 2.298755186721992e-05,
      "loss": 0.34653487205505373,
      "step": 170
    },
    {
      "epoch": 0.7468879668049793,
      "grad_norm": 0.1836974024772644,
      "learning_rate": 2.2572614107883818e-05,
      "loss": 0.3778273582458496,
      "step": 180
    },
    {
      "epoch": 0.7883817427385892,
      "grad_norm": 3.657994270324707,
      "learning_rate": 2.215767634854772e-05,
      "loss": 0.278352952003479,
      "step": 190
    },
    {
      "epoch": 0.8298755186721992,
      "grad_norm": 7.022721290588379,
      "learning_rate": 2.1742738589211617e-05,
      "loss": 0.2878831386566162,
      "step": 200
    },
    {
      "epoch": 0.8713692946058091,
      "grad_norm": 7.4005513191223145,
      "learning_rate": 2.1327800829875522e-05,
      "loss": 0.4999371528625488,
      "step": 210
    },
    {
      "epoch": 0.9128630705394191,
      "grad_norm": 4.301907062530518,
      "learning_rate": 2.091286307053942e-05,
      "loss": 0.22429072856903076,
      "step": 220
    },
    {
      "epoch": 0.9543568464730291,
      "grad_norm": 5.327391624450684,
      "learning_rate": 2.049792531120332e-05,
      "loss": 0.1804206371307373,
      "step": 230
    },
    {
      "epoch": 0.995850622406639,
      "grad_norm": 0.04573235660791397,
      "learning_rate": 2.008298755186722e-05,
      "loss": 0.2869739294052124,
      "step": 240
    },
    {
      "epoch": 1.0,
      "eval_loss": 0.3735365867614746,
      "eval_runtime": 24.9099,
      "eval_samples_per_second": 13.649,
      "eval_steps_per_second": 1.726,
      "step": 241
    },
    {
      "epoch": 1.037344398340249,
      "grad_norm": 0.8284101486206055,
      "learning_rate": 1.966804979253112e-05,
      "loss": 0.14156702756881714,
      "step": 250
    },
    {
      "epoch": 1.0788381742738589,
      "grad_norm": 1.526591420173645,
      "learning_rate": 1.925311203319502e-05,
      "loss": 0.14006739854812622,
      "step": 260
    },
    {
      "epoch": 1.120331950207469,
      "grad_norm": 0.7837855815887451,
      "learning_rate": 1.8838174273858922e-05,
      "loss": 0.11302624940872193,
      "step": 270
    },
    {
      "epoch": 1.161825726141079,
      "grad_norm": 0.6723598837852478,
      "learning_rate": 1.8423236514522824e-05,
      "loss": 0.0597973644733429,
      "step": 280
    },
    {
      "epoch": 1.2033195020746887,
      "grad_norm": 3.216686964035034,
      "learning_rate": 1.800829875518672e-05,
      "loss": 0.42694454193115233,
      "step": 290
    },
    {
      "epoch": 1.2448132780082988,
      "grad_norm": 4.22862434387207,
      "learning_rate": 1.7593360995850623e-05,
      "loss": 0.31748228073120116,
      "step": 300
    },
    {
      "epoch": 1.2863070539419086,
      "grad_norm": 0.05735517665743828,
      "learning_rate": 1.717842323651452e-05,
      "loss": 0.44448189735412597,
      "step": 310
    },
    {
      "epoch": 1.3278008298755186,
      "grad_norm": 0.001453970093280077,
      "learning_rate": 1.6763485477178425e-05,
      "loss": 0.33296990394592285,
      "step": 320
    },
    {
      "epoch": 1.3692946058091287,
      "grad_norm": 0.5776257514953613,
      "learning_rate": 1.6348547717842323e-05,
      "loss": 0.15204286575317383,
      "step": 330
    },
    {
      "epoch": 1.4107883817427385,
      "grad_norm": 0.16427955031394958,
      "learning_rate": 1.5933609958506224e-05,
      "loss": 0.21013164520263672,
      "step": 340
    },
    {
      "epoch": 1.4522821576763485,
      "grad_norm": 6.358363151550293,
      "learning_rate": 1.5518672199170126e-05,
      "loss": 0.23987360000610353,
      "step": 350
    },
    {
      "epoch": 1.4937759336099585,
      "grad_norm": 5.248785018920898,
      "learning_rate": 1.5103734439834025e-05,
      "loss": 0.4400056838989258,
      "step": 360
    },
    {
      "epoch": 1.5352697095435683,
      "grad_norm": 1.6565955877304077,
      "learning_rate": 1.4688796680497925e-05,
      "loss": 0.5957468509674072,
      "step": 370
    },
    {
      "epoch": 1.5767634854771784,
      "grad_norm": 0.6148923635482788,
      "learning_rate": 1.4273858921161826e-05,
      "loss": 0.19119338989257811,
      "step": 380
    },
    {
      "epoch": 1.6182572614107884,
      "grad_norm": 0.37317121028900146,
      "learning_rate": 1.3858921161825727e-05,
      "loss": 0.34333052635192873,
      "step": 390
    },
    {
      "epoch": 1.6597510373443982,
      "grad_norm": 0.026372672989964485,
      "learning_rate": 1.3443983402489627e-05,
      "loss": 0.3671098232269287,
      "step": 400
    },
    {
      "epoch": 1.7012448132780082,
      "grad_norm": 0.008655710145831108,
      "learning_rate": 1.3029045643153528e-05,
      "loss": 0.19620198011398315,
      "step": 410
    },
    {
      "epoch": 1.7427385892116183,
      "grad_norm": 5.18820333480835,
      "learning_rate": 1.2614107883817427e-05,
      "loss": 0.23198916912078857,
      "step": 420
    },
    {
      "epoch": 1.784232365145228,
      "grad_norm": 2.622978687286377,
      "learning_rate": 1.2199170124481329e-05,
      "loss": 0.1761869192123413,
      "step": 430
    },
    {
      "epoch": 1.8257261410788381,
      "grad_norm": 0.21988818049430847,
      "learning_rate": 1.1784232365145228e-05,
      "loss": 0.30829241275787356,
      "step": 440
    },
    {
      "epoch": 1.8672199170124482,
      "grad_norm": 2.605598211288452,
      "learning_rate": 1.1369294605809128e-05,
      "loss": 0.29313092231750487,
      "step": 450
    },
    {
      "epoch": 1.908713692946058,
      "grad_norm": 0.01529083214700222,
      "learning_rate": 1.0954356846473029e-05,
      "loss": 0.08202864527702332,
      "step": 460
    },
    {
      "epoch": 1.950207468879668,
      "grad_norm": 0.27508100867271423,
      "learning_rate": 1.0539419087136929e-05,
      "loss": 0.17424751520156861,
      "step": 470
    },
    {
      "epoch": 1.991701244813278,
      "grad_norm": 8.90056324005127,
      "learning_rate": 1.0124481327800831e-05,
      "loss": 0.4619795799255371,
      "step": 480
    },
    {
      "epoch": 2.0,
      "eval_loss": 0.32365676760673523,
      "eval_runtime": 26.923,
      "eval_samples_per_second": 12.629,
      "eval_steps_per_second": 1.597,
      "step": 482
    }
  ],
  "logging_steps": 10,
  "max_steps": 723,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 276767473188960.0,
  "train_batch_size": 8,
  "trial_name": null,
  "trial_params": null
}
