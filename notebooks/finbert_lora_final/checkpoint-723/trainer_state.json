{
  "best_global_step": 723,
  "best_metric": 0.30350011587142944,
  "best_model_checkpoint": "./finbert_lora_final\\checkpoint-723",
  "epoch": 3.0,
  "eval_steps": 500,
  "global_step": 723,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.04149377593360996,
      "grad_norm": 0.07839247584342957,
      "learning_rate": 2.9626556016597514e-05,
      "loss": 0.5458790302276612,
      "step": 10
    },
    {
      "epoch": 0.08298755186721991,
      "grad_norm": 0.005237376783043146,
      "learning_rate": 2.9211618257261412e-05,
      "loss": 0.48991737365722654,
      "step": 20
    },
    {
      "epoch": 0.12448132780082988,
      "grad_norm": 1.6888333559036255,
      "learning_rate": 2.8796680497925313e-05,
      "loss": 0.35935797691345217,
      "step": 30
    },
    {
      "epoch": 0.16597510373443983,
      "grad_norm": 4.850772380828857,
      "learning_rate": 2.838174273858921e-05,
      "loss": 0.640938663482666,
      "step": 40
    },
    {
      "epoch": 0.2074688796680498,
      "grad_norm": 0.09707718342542648,
      "learning_rate": 2.7966804979253112e-05,
      "loss": 0.3962913274765015,
      "step": 50
    },
    {
      "epoch": 0.24896265560165975,
      "grad_norm": 0.0555955246090889,
      "learning_rate": 2.7551867219917013e-05,
      "loss": 0.08887227773666381,
      "step": 60
    },
    {
      "epoch": 0.29045643153526973,
      "grad_norm": 8.862092018127441,
      "learning_rate": 2.7136929460580915e-05,
      "loss": 0.2729800701141357,
      "step": 70
    },
    {
      "epoch": 0.33195020746887965,
      "grad_norm": 11.323979377746582,
      "learning_rate": 2.6721991701244816e-05,
      "loss": 0.5787821769714355,
      "step": 80
    },
    {
      "epoch": 0.37344398340248963,
      "grad_norm": 5.134685039520264,
      "learning_rate": 2.6307053941908714e-05,
      "loss": 0.46543326377868655,
      "step": 90
    },
    {
      "epoch": 0.4149377593360996,
      "grad_norm": 0.0032711250241845846,
      "learning_rate": 2.5892116182572615e-05,
      "loss": 0.1333439588546753,
      "step": 100
    },
    {
      "epoch": 0.45643153526970953,
      "grad_norm": 0.23824091255664825,
      "learning_rate": 2.5477178423236516e-05,
      "loss": 0.2031480073928833,
      "step": 110
    },
    {
      "epoch": 0.4979253112033195,
      "grad_norm": 0.04118388518691063,
      "learning_rate": 2.5062240663900417e-05,
      "loss": 0.6033592224121094,
      "step": 120
    },
    {
      "epoch": 0.5394190871369294,
      "grad_norm": 7.07978630065918,
      "learning_rate": 2.4647302904564315e-05,
      "loss": 0.5177162647247314,
      "step": 130
    },
    {
      "epoch": 0.5809128630705395,
      "grad_norm": 6.6420979499816895,
      "learning_rate": 2.4232365145228216e-05,
      "loss": 0.2990450382232666,
      "step": 140
    },
    {
      "epoch": 0.6224066390041494,
      "grad_norm": 5.971555233001709,
      "learning_rate": 2.3817427385892114e-05,
      "loss": 0.24831981658935548,
      "step": 150
    },
    {
      "epoch": 0.6639004149377593,
      "grad_norm": 0.0477827787399292,
      "learning_rate": 2.340248962655602e-05,
      "loss": 0.1218605637550354,
      "step": 160
    },
    {
      "epoch": 0.7053941908713693,
      "grad_norm": 4.849154472351074,
      "learning_rate": 2.298755186721992e-05,
      "loss": 0.34653487205505373,
      "step": 170
    },
    {
      "epoch": 0.7468879668049793,
      "grad_norm": 0.1836974024772644,
      "learning_rate": 2.2572614107883818e-05,
      "loss": 0.3778273582458496,
      "step": 180
    },
    {
      "epoch": 0.7883817427385892,
      "grad_norm": 3.657994270324707,
      "learning_rate": 2.215767634854772e-05,
      "loss": 0.278352952003479,
      "step": 190
    },
    {
      "epoch": 0.8298755186721992,
      "grad_norm": 7.022721290588379,
      "learning_rate": 2.1742738589211617e-05,
      "loss": 0.2878831386566162,
      "step": 200
    },
    {
      "epoch": 0.8713692946058091,
      "grad_norm": 7.4005513191223145,
      "learning_rate": 2.1327800829875522e-05,
      "loss": 0.4999371528625488,
      "step": 210
    },
    {
      "epoch": 0.9128630705394191,
      "grad_norm": 4.301907062530518,
      "learning_rate": 2.091286307053942e-05,
      "loss": 0.22429072856903076,
      "step": 220
    },
    {
      "epoch": 0.9543568464730291,
      "grad_norm": 5.327391624450684,
      "learning_rate": 2.049792531120332e-05,
      "loss": 0.1804206371307373,
      "step": 230
    },
    {
      "epoch": 0.995850622406639,
      "grad_norm": 0.04573235660791397,
      "learning_rate": 2.008298755186722e-05,
      "loss": 0.2869739294052124,
      "step": 240
    },
    {
      "epoch": 1.0,
      "eval_loss": 0.3735365867614746,
      "eval_runtime": 24.9099,
      "eval_samples_per_second": 13.649,
      "eval_steps_per_second": 1.726,
      "step": 241
    },
    {
      "epoch": 1.037344398340249,
      "grad_norm": 0.8284101486206055,
      "learning_rate": 1.966804979253112e-05,
      "loss": 0.14156702756881714,
      "step": 250
    },
    {
      "epoch": 1.0788381742738589,
      "grad_norm": 1.526591420173645,
      "learning_rate": 1.925311203319502e-05,
      "loss": 0.14006739854812622,
      "step": 260
    },
    {
      "epoch": 1.120331950207469,
      "grad_norm": 0.7837855815887451,
      "learning_rate": 1.8838174273858922e-05,
      "loss": 0.11302624940872193,
      "step": 270
    },
    {
      "epoch": 1.161825726141079,
      "grad_norm": 0.6723598837852478,
      "learning_rate": 1.8423236514522824e-05,
      "loss": 0.0597973644733429,
      "step": 280
    },
    {
      "epoch": 1.2033195020746887,
      "grad_norm": 3.216686964035034,
      "learning_rate": 1.800829875518672e-05,
      "loss": 0.42694454193115233,
      "step": 290
    },
    {
      "epoch": 1.2448132780082988,
      "grad_norm": 4.22862434387207,
      "learning_rate": 1.7593360995850623e-05,
      "loss": 0.31748228073120116,
      "step": 300
    },
    {
      "epoch": 1.2863070539419086,
      "grad_norm": 0.05735517665743828,
      "learning_rate": 1.717842323651452e-05,
      "loss": 0.44448189735412597,
      "step": 310
    },
    {
      "epoch": 1.3278008298755186,
      "grad_norm": 0.001453970093280077,
      "learning_rate": 1.6763485477178425e-05,
      "loss": 0.33296990394592285,
      "step": 320
    },
    {
      "epoch": 1.3692946058091287,
      "grad_norm": 0.5776257514953613,
      "learning_rate": 1.6348547717842323e-05,
      "loss": 0.15204286575317383,
      "step": 330
    },
    {
      "epoch": 1.4107883817427385,
      "grad_norm": 0.16427955031394958,
      "learning_rate": 1.5933609958506224e-05,
      "loss": 0.21013164520263672,
      "step": 340
    },
    {
      "epoch": 1.4522821576763485,
      "grad_norm": 6.358363151550293,
      "learning_rate": 1.5518672199170126e-05,
      "loss": 0.23987360000610353,
      "step": 350
    },
    {
      "epoch": 1.4937759336099585,
      "grad_norm": 5.248785018920898,
      "learning_rate": 1.5103734439834025e-05,
      "loss": 0.4400056838989258,
      "step": 360
    },
    {
      "epoch": 1.5352697095435683,
      "grad_norm": 1.6565955877304077,
      "learning_rate": 1.4688796680497925e-05,
      "loss": 0.5957468509674072,
      "step": 370
    },
    {
      "epoch": 1.5767634854771784,
      "grad_norm": 0.6148923635482788,
      "learning_rate": 1.4273858921161826e-05,
      "loss": 0.19119338989257811,
      "step": 380
    },
    {
      "epoch": 1.6182572614107884,
      "grad_norm": 0.37317121028900146,
      "learning_rate": 1.3858921161825727e-05,
      "loss": 0.34333052635192873,
      "step": 390
    },
    {
      "epoch": 1.6597510373443982,
      "grad_norm": 0.026372672989964485,
      "learning_rate": 1.3443983402489627e-05,
      "loss": 0.3671098232269287,
      "step": 400
    },
    {
      "epoch": 1.7012448132780082,
      "grad_norm": 0.008655710145831108,
      "learning_rate": 1.3029045643153528e-05,
      "loss": 0.19620198011398315,
      "step": 410
    },
    {
      "epoch": 1.7427385892116183,
      "grad_norm": 5.18820333480835,
      "learning_rate": 1.2614107883817427e-05,
      "loss": 0.23198916912078857,
      "step": 420
    },
    {
      "epoch": 1.784232365145228,
      "grad_norm": 2.622978687286377,
      "learning_rate": 1.2199170124481329e-05,
      "loss": 0.1761869192123413,
      "step": 430
    },
    {
      "epoch": 1.8257261410788381,
      "grad_norm": 0.21988818049430847,
      "learning_rate": 1.1784232365145228e-05,
      "loss": 0.30829241275787356,
      "step": 440
    },
    {
      "epoch": 1.8672199170124482,
      "grad_norm": 2.605598211288452,
      "learning_rate": 1.1369294605809128e-05,
      "loss": 0.29313092231750487,
      "step": 450
    },
    {
      "epoch": 1.908713692946058,
      "grad_norm": 0.01529083214700222,
      "learning_rate": 1.0954356846473029e-05,
      "loss": 0.08202864527702332,
      "step": 460
    },
    {
      "epoch": 1.950207468879668,
      "grad_norm": 0.27508100867271423,
      "learning_rate": 1.0539419087136929e-05,
      "loss": 0.17424751520156861,
      "step": 470
    },
    {
      "epoch": 1.991701244813278,
      "grad_norm": 8.90056324005127,
      "learning_rate": 1.0124481327800831e-05,
      "loss": 0.4619795799255371,
      "step": 480
    },
    {
      "epoch": 2.0,
      "eval_loss": 0.32365676760673523,
      "eval_runtime": 26.923,
      "eval_samples_per_second": 12.629,
      "eval_steps_per_second": 1.597,
      "step": 482
    },
    {
      "epoch": 2.033195020746888,
      "grad_norm": 4.84116792678833,
      "learning_rate": 9.709543568464731e-06,
      "loss": 0.16859015226364135,
      "step": 490
    },
    {
      "epoch": 2.074688796680498,
      "grad_norm": 0.5036858916282654,
      "learning_rate": 9.29460580912863e-06,
      "loss": 0.1704317331314087,
      "step": 500
    },
    {
      "epoch": 2.116182572614108,
      "grad_norm": 0.2337976098060608,
      "learning_rate": 8.879668049792532e-06,
      "loss": 0.05285539031028748,
      "step": 510
    },
    {
      "epoch": 2.1576763485477177,
      "grad_norm": 5.562930107116699,
      "learning_rate": 8.464730290456431e-06,
      "loss": 0.32570505142211914,
      "step": 520
    },
    {
      "epoch": 2.199170124481328,
      "grad_norm": 8.623848915100098,
      "learning_rate": 8.049792531120333e-06,
      "loss": 0.2912020206451416,
      "step": 530
    },
    {
      "epoch": 2.240663900414938,
      "grad_norm": 9.360579490661621,
      "learning_rate": 7.634854771784232e-06,
      "loss": 0.25171546936035155,
      "step": 540
    },
    {
      "epoch": 2.2821576763485476,
      "grad_norm": 6.130461692810059,
      "learning_rate": 7.219917012448133e-06,
      "loss": 0.3453659534454346,
      "step": 550
    },
    {
      "epoch": 2.323651452282158,
      "grad_norm": 4.882106304168701,
      "learning_rate": 6.804979253112034e-06,
      "loss": 0.18090131282806396,
      "step": 560
    },
    {
      "epoch": 2.3651452282157677,
      "grad_norm": 6.771122932434082,
      "learning_rate": 6.390041493775933e-06,
      "loss": 0.16624295711517334,
      "step": 570
    },
    {
      "epoch": 2.4066390041493775,
      "grad_norm": 10.979804039001465,
      "learning_rate": 5.975103734439834e-06,
      "loss": 0.2662170648574829,
      "step": 580
    },
    {
      "epoch": 2.4481327800829877,
      "grad_norm": 5.636918544769287,
      "learning_rate": 5.560165975103735e-06,
      "loss": 0.22021713256835937,
      "step": 590
    },
    {
      "epoch": 2.4896265560165975,
      "grad_norm": 5.479887962341309,
      "learning_rate": 5.145228215767635e-06,
      "loss": 0.12714483737945556,
      "step": 600
    },
    {
      "epoch": 2.5311203319502074,
      "grad_norm": 0.2878391444683075,
      "learning_rate": 4.730290456431535e-06,
      "loss": 0.3158209800720215,
      "step": 610
    },
    {
      "epoch": 2.572614107883817,
      "grad_norm": 0.11873046308755875,
      "learning_rate": 4.315352697095435e-06,
      "loss": 0.3029896020889282,
      "step": 620
    },
    {
      "epoch": 2.6141078838174274,
      "grad_norm": 0.5170396566390991,
      "learning_rate": 3.9004149377593365e-06,
      "loss": 0.11318351030349731,
      "step": 630
    },
    {
      "epoch": 2.6556016597510372,
      "grad_norm": 0.04303627461194992,
      "learning_rate": 3.485477178423237e-06,
      "loss": 0.27197980880737305,
      "step": 640
    },
    {
      "epoch": 2.6970954356846475,
      "grad_norm": 2.3646085262298584,
      "learning_rate": 3.070539419087137e-06,
      "loss": 0.14981874227523803,
      "step": 650
    },
    {
      "epoch": 2.7385892116182573,
      "grad_norm": 1.6839687824249268,
      "learning_rate": 2.6556016597510377e-06,
      "loss": 0.5010355949401856,
      "step": 660
    },
    {
      "epoch": 2.780082987551867,
      "grad_norm": 0.5585474371910095,
      "learning_rate": 2.2406639004149376e-06,
      "loss": 0.15828779935836793,
      "step": 670
    },
    {
      "epoch": 2.821576763485477,
      "grad_norm": 6.869863033294678,
      "learning_rate": 1.8257261410788382e-06,
      "loss": 0.30019102096557615,
      "step": 680
    },
    {
      "epoch": 2.863070539419087,
      "grad_norm": 5.06916618347168,
      "learning_rate": 1.4107883817427386e-06,
      "loss": 0.20657811164855958,
      "step": 690
    },
    {
      "epoch": 2.904564315352697,
      "grad_norm": 0.021002566441893578,
      "learning_rate": 9.95850622406639e-07,
      "loss": 0.06507411003112792,
      "step": 700
    },
    {
      "epoch": 2.9460580912863072,
      "grad_norm": 0.8267587423324585,
      "learning_rate": 5.809128630705394e-07,
      "loss": 0.1955951929092407,
      "step": 710
    },
    {
      "epoch": 2.987551867219917,
      "grad_norm": 0.003936966881155968,
      "learning_rate": 1.6597510373443985e-07,
      "loss": 0.21370880603790282,
      "step": 720
    },
    {
      "epoch": 3.0,
      "eval_loss": 0.30350011587142944,
      "eval_runtime": 24.1216,
      "eval_samples_per_second": 14.095,
      "eval_steps_per_second": 1.783,
      "step": 723
    }
  ],
  "logging_steps": 10,
  "max_steps": 723,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 414795206811600.0,
  "train_batch_size": 8,
  "trial_name": null,
  "trial_params": null
}
